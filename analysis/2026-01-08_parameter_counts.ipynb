{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "download: s3://measurement-noise-scaling-laws/data/larry/100000/1.0/results/Geneformer/checkpoint-63000/model.safetensors to outputs/2026-01-08_parameter_counts/larry/model.safetensors\n",
            "download: s3://measurement-noise-scaling-laws/data/larry/100000/1.0/results/SCVI/model/model.pt to outputs/2026-01-08_parameter_counts/larry/model.pt\n",
            "download: s3://measurement-noise-scaling-laws/data/merfish/60000/1.0/results/Geneformer/checkpoint-65000/model.safetensors to outputs/2026-01-08_parameter_counts/merfish/model.safetensors\n",
            "download: s3://measurement-noise-scaling-laws/data/merfish/60000/1.0/results/SCVI/model/model.pt to outputs/2026-01-08_parameter_counts/merfish/model.pt\n",
            "download: s3://measurement-noise-scaling-laws/data/PBMC/100000/1.0/results/Geneformer/checkpoint-42000/model.safetensors to outputs/2026-01-08_parameter_counts/pbmc/model.safetensors\n",
            "download: s3://measurement-noise-scaling-laws/data/PBMC/100000/1.0/results/SCVI/model/model.pt to outputs/2026-01-08_parameter_counts/pbmc/model.pt\n",
            "download: s3://measurement-noise-scaling-laws/data/shendure/10000000/1.0/results/Geneformer/checkpoint-86000/model.safetensors to outputs/2026-01-08_parameter_counts/shendure/model.safetensors\n",
            "download: s3://measurement-noise-scaling-laws/data/shendure/59948/1.0/results/SCVI/model/model.pt to outputs/2026-01-08_parameter_counts/shendure/model.pt\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "s3_paths = [\n",
        "    \"s3://measurement-noise-scaling-laws/data/larry/100000/1.0/results/Geneformer/checkpoint-63000/model.safetensors\",\n",
        "    \"s3://measurement-noise-scaling-laws/data/larry/100000/1.0/results/SCVI/model/model.pt\",\n",
        "    \"s3://measurement-noise-scaling-laws/data/merfish/60000/1.0/results/Geneformer/checkpoint-65000/model.safetensors\",\n",
        "    \"s3://measurement-noise-scaling-laws/data/merfish/60000/1.0/results/SCVI/model/model.pt\",\n",
        "    \"s3://measurement-noise-scaling-laws/data/PBMC/100000/1.0/results/Geneformer/checkpoint-42000/model.safetensors\",\n",
        "    \"s3://measurement-noise-scaling-laws/data/PBMC/100000/1.0/results/SCVI/model/model.pt\",\n",
        "    \"s3://measurement-noise-scaling-laws/data/shendure/10000000/1.0/results/Geneformer/checkpoint-86000/model.safetensors\",\n",
        "    \"s3://measurement-noise-scaling-laws/data/shendure/59948/1.0/results/SCVI/model/model.pt\",\n",
        "]\n",
        "\n",
        "output_dir = \"outputs/2026-01-08_parameter_counts\"\n",
        "import os\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for s3_path in s3_paths:\n",
        "    # Extract the filename from the S3 path\n",
        "    filename = s3_path.split(\"/\")[-1]\n",
        "    # Find the dataset folder (immediately after \"data/\")\n",
        "    parts = s3_path.split('/')\n",
        "    if \"data\" in parts:\n",
        "        data_index = parts.index(\"data\")\n",
        "        # assume next part is dataset (case insensitive in output on disk)\n",
        "        dataset = parts[data_index + 1].lower()\n",
        "    else:\n",
        "        dataset = \"unknown\"\n",
        "    # Build destination folder\n",
        "    dataset_dir = os.path.join(output_dir, dataset)\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "    dest_path = os.path.join(dataset_dir, filename)\n",
        "    subprocess.run([\"aws\", \"s3\", \"cp\", s3_path, dest_path])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import csv\n",
        "\n",
        "def count_params(model):\n",
        "    # Try common keys for state dicts\n",
        "    if isinstance(model, dict):\n",
        "        if 'model_state_dict' in model:\n",
        "            state_dict = model['model_state_dict']\n",
        "        elif 'state_dict' in model:\n",
        "            state_dict = model['state_dict']\n",
        "        else:\n",
        "            # Fallback: take first tensor-dict found\n",
        "            for v in model.values():\n",
        "                if isinstance(v, dict):\n",
        "                    for possible_sd in ['model_state_dict', 'state_dict']:\n",
        "                        if possible_sd in v:\n",
        "                            state_dict = v[possible_sd]\n",
        "                            break\n",
        "                    else:\n",
        "                        continue\n",
        "                    break\n",
        "            else:\n",
        "                state_dict = None\n",
        "    elif hasattr(model, 'state_dict'):\n",
        "        state_dict = model.state_dict()\n",
        "    else:\n",
        "        state_dict = None\n",
        "\n",
        "    if state_dict is None:\n",
        "        raise ValueError(\"Could not find model state dict to count parameters.\")\n",
        "\n",
        "    return sum(v.numel() for v in state_dict.values() if hasattr(v, \"numel\"))\n",
        "\n",
        "def count_parameters_safetensors(safetensors_path):\n",
        "    from safetensors.torch import load_file\n",
        "    state_dict = load_file(safetensors_path)\n",
        "    total = sum(p.numel() for p in state_dict.values())\n",
        "    return total\n",
        "\n",
        "base_dir = os.path.join(\"outputs\", \"2026-01-08_parameter_counts\")\n",
        "csv_output_path = os.path.join(base_dir, \"parameter_counts.csv\")\n",
        "rows = []\n",
        "\n",
        "try:\n",
        "    import safetensors.torch  # noqa: F401\n",
        "    safetensors_available = True\n",
        "except ImportError:\n",
        "    safetensors_available = False\n",
        "\n",
        "for dataset_folder in sorted(os.listdir(base_dir)):\n",
        "    dataset_path = os.path.join(base_dir, dataset_folder)\n",
        "    if not os.path.isdir(dataset_path):\n",
        "        continue\n",
        "\n",
        "    pt_path = os.path.join(dataset_path, \"model.pt\")\n",
        "    safetensors_path = os.path.join(dataset_path, \"model.safetensors\")\n",
        "\n",
        "    # Process svi (model.pt)\n",
        "    if os.path.exists(pt_path):\n",
        "        import torch\n",
        "        try:\n",
        "            model = torch.load(pt_path, map_location=\"cpu\", weights_only=False)\n",
        "            pt_params = count_params(model)\n",
        "            rows.append({\n",
        "                \"dataset\": dataset_folder,\n",
        "                \"model\": \"svi\",\n",
        "                \"params\": pt_params\n",
        "            })\n",
        "        except Exception as e:\n",
        "            rows.append({\n",
        "                \"dataset\": dataset_folder,\n",
        "                \"model\": \"svi\",\n",
        "                \"params\": None\n",
        "            })\n",
        "    # Process geneformer (model.safetensors)\n",
        "    if os.path.exists(safetensors_path) and safetensors_available:\n",
        "        try:\n",
        "            safetensor_params = count_parameters_safetensors(safetensors_path)\n",
        "            rows.append({\n",
        "                \"dataset\": dataset_folder,\n",
        "                \"model\": \"geneformer\",\n",
        "                \"params\": safetensor_params\n",
        "            })\n",
        "        except Exception as e:\n",
        "            rows.append({\n",
        "                \"dataset\": dataset_folder,\n",
        "                \"model\": \"geneformer\",\n",
        "                \"params\": None\n",
        "            })\n",
        "    elif os.path.exists(safetensors_path) and not safetensors_available:\n",
        "        rows.append({\n",
        "            \"dataset\": dataset_folder,\n",
        "            \"model\": \"geneformer\",\n",
        "            \"params\": None\n",
        "        })\n",
        "\n",
        "# Write to CSV with columns: dataset, model, params\n",
        "with open(csv_output_path, mode=\"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"dataset\", \"model\", \"params\"])\n",
        "    writer.writeheader()\n",
        "    for row in rows:\n",
        "        writer.writerow(row)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     total_parameters  train/learning_rate  eval/steps_per_second  \\\n",
            "0          13480151.0                  NaN                    NaN   \n",
            "1                 NaN             0.000200                    NaN   \n",
            "2                 NaN                  NaN                  2.404   \n",
            "3                 NaN             0.000400                    NaN   \n",
            "4                 NaN                  NaN                  2.521   \n",
            "..                ...                  ...                    ...   \n",
            "171               NaN             0.000948                    NaN   \n",
            "172               NaN                  NaN                  2.510   \n",
            "173               NaN             0.000947                    NaN   \n",
            "174               NaN                  NaN                  4.543   \n",
            "175               NaN                  NaN                    NaN   \n",
            "\n",
            "     eval/samples_per_second  _step    _timestamp  train/grad_norm  eval/loss  \\\n",
            "0                        NaN      0  1.764008e+09              NaN        NaN   \n",
            "1                        NaN      1  1.764008e+09         0.721820        NaN   \n",
            "2                    240.434      2  1.764008e+09              NaN   8.816442   \n",
            "3                        NaN      3  1.764008e+09         1.653340        NaN   \n",
            "4                    252.094      4  1.764008e+09              NaN   6.754172   \n",
            "..                       ...    ...           ...              ...        ...   \n",
            "171                      NaN    171  1.764015e+09         0.498337        NaN   \n",
            "172                  251.049    172  1.764015e+09              NaN   3.678129   \n",
            "173                      NaN    173  1.764015e+09         0.435879        NaN   \n",
            "174                  454.311    174  1.764015e+09              NaN   3.672816   \n",
            "175                      NaN    175  1.764015e+09              NaN        NaN   \n",
            "\n",
            "     train/epoch     _runtime  train/loss  eval/runtime  train/global_step  \\\n",
            "0            NaN     1.445993         NaN           NaN                NaN   \n",
            "1         0.0064    89.736780      9.4659           NaN             1000.0   \n",
            "2         0.0064    93.897966         NaN        4.1591             1000.0   \n",
            "3         0.0128   186.924969      7.8926           NaN             2000.0   \n",
            "4         0.0128   190.893906         NaN        3.9668             2000.0   \n",
            "..           ...          ...         ...           ...                ...   \n",
            "171       0.5504  6596.380743      3.6911           NaN            86000.0   \n",
            "172       0.5504  6600.366131         NaN        3.9833            86000.0   \n",
            "173       0.5568  6669.071201      3.6931           NaN            87000.0   \n",
            "174       0.5568  6671.274050         NaN        2.2011            87000.0   \n",
            "175       0.5568  6671.490141         NaN           NaN            87000.0   \n",
            "\n",
            "     parameters_millions  \n",
            "0              13.480151  \n",
            "1                    NaN  \n",
            "2                    NaN  \n",
            "3                    NaN  \n",
            "4                    NaN  \n",
            "..                   ...  \n",
            "171                  NaN  \n",
            "172                  NaN  \n",
            "173                  NaN  \n",
            "174                  NaN  \n",
            "175                  NaN  \n",
            "\n",
            "[176 rows x 14 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from wandb import Api\n",
        "\n",
        "# Download all W&B run history (all columns)\n",
        "api = Api()\n",
        "run = api.run('igor-somite-somite/geneformer-scaling-laws/id3ize6p')\n",
        "df = pd.DataFrame(list(run.scan_history()))\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Geneformer params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_params count 13480151.0\n"
          ]
        }
      ],
      "source": [
        "print(\"total_params count\", df['total_parameters'].loc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SCVI params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = Api()\n",
        "run = api.run('igor-somite-somite/scvi-scaling-laws/kse2pfsd')\n",
        "df_scvi = pd.DataFrame(list(run.scan_history()))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
