{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c80c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c707f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise levels: [9.99000999e-01 9.90099010e-01 9.09090909e-01 5.00000000e-01\n",
      " 9.09090909e-02 9.90099010e-03 9.99000999e-04 9.99900010e-05\n",
      " 9.99990000e-06 9.99999000e-07]\n",
      "(0.999001, 0.990099, 0.909091, 0.500000, 0.090909, 0.009901, 0.000999, 0.000100, 0.000010, 0.000001)\n",
      "Parsing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a129c6fd484fc789dd07bd8def2a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collecting sequences:   0%|          | 0/100000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 63374 sequences across 71 unique months\n",
      "Months: [-109, -78, -29, -10, -7, -3, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "Train: 47530, Test: 15844\n",
      "Unique months in test set: [-109  -78   -1    0    1    2    3    4    5    6    7    8    9   10\n",
      "   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n",
      "   25   26   27   28   29   30   31   32   33   34   35   36   37   38\n",
      "   39   40   41   42   43   44   45   46   47   48   49   50   51   52\n",
      "   53   54   55   56   57   58   59   60   61   62   63]\n",
      "\n",
      "Tokenizing and saving data for each noise level...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cec8934589943a88d6aa8e180bbc223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Noise levels:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing noise level: 0.999001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626409e1d2d249a0ba6ee87f3db27290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.999001):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1395b79f869f4f4c8171c816858bea70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.999001):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.999001.pt\n",
      "\n",
      "Processing noise level: 0.990099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e281ffff66a84231915b08f6a84e095e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.990099):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11213a79dd524353a148972170b267b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.990099):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.990099.pt\n",
      "\n",
      "Processing noise level: 0.909091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2038ae41c6854a57a93809f5ba543965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.909091):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc165f0465304cde8ca87b94afdb97fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.909091):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.909091.pt\n",
      "\n",
      "Processing noise level: 0.500000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d2619095bf482085c78e74670267fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.500000):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead6914727be4913a05e545dcca2b0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.500000):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.500000.pt\n",
      "\n",
      "Processing noise level: 0.090909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d751e8d8f647f3be5a2197cdd1ea69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.090909):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0001aeed2ba44ea0864ef4d625bd32c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.090909):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.090909.pt\n",
      "\n",
      "Processing noise level: 0.009901\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bb9c36a7a5465aa3a24de6148d4773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.009901):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b911d70b53d04814b28c9b7a2eede542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.009901):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.009901.pt\n",
      "\n",
      "Processing noise level: 0.000999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd29516103a49c9aeb133e494935b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.000999):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65b6f3f7da54bf9ace69ab68b8137d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.000999):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.000999.pt\n",
      "\n",
      "Processing noise level: 0.000100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4602d0104f48c6a06d744aaa290b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.000100):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b481acafc74e279fda570d9f23d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.000100):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.000100.pt\n",
      "\n",
      "Processing noise level: 0.000010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9ddef81477471c97c06db7387eb1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.000010):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0af32f9da0491884e13f762ce13f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.000010):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.000010.pt\n",
      "\n",
      "Processing noise level: 0.000001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5042eff2b34ddd8fc7f7dfd3223d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.000001):   0%|          | 0/47530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e6136efe4c46b5b5c9d229268a5972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mutating (rate=0.000001):   0%|          | 0/15844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing train data...\n",
      "  Tokenizing test data...\n",
      "  Saved to seq/data_noise_0.000001.pt\n",
      "\\data preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "fn = '../DistributionEmbeddings/data/spikeprot0430/spikeprot0430.fasta'\n",
    "ratios = np.logspace(-3, 6, 10)\n",
    "noise_levels = 1 / (ratios+1)\n",
    "print(\"Noise levels:\", noise_levels)\n",
    "print(\"(\"+\", \".join([f\"{nl:.6f}\" for nl in noise_levels])+\")\")\n",
    "\n",
    "aas = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "def parse_data(path, lines_to_read=10**8, max_per_month=1000):\n",
    "    \"\"\"Parse data with balanced sampling across months.\"\"\"\n",
    "    seqs_by_month = defaultdict(list)\n",
    "    iterator = SeqIO.parse(path, \"fasta\")\n",
    "    \n",
    "    for _ in tqdm(range(lines_to_read), desc='collecting sequences'):\n",
    "        try:\n",
    "            r = next(iterator)\n",
    "            fields = r.description.split(\"|\")\n",
    "            date = (fields + [\"?\"] * 11)[2]\n",
    "            \n",
    "            # filter valid dates\n",
    "            if len(date) == 10 and date[4] == '-' and date[5:7] != '00' and date[-2:] != '00':\n",
    "                yyyy_mm = date[:7]\n",
    "                months_since_2020 = (int(yyyy_mm[:4]) - 2020) * 12 + int(yyyy_mm[5:7]) - 1\n",
    "                \n",
    "                # Only add if we haven't hit the cap for this month\n",
    "                if len(seqs_by_month[months_since_2020]) < max_per_month:\n",
    "                    seqs_by_month[months_since_2020].append(str(r.seq))\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Flatten to lists\n",
    "    seqs, months = [], []\n",
    "    for month_val, month_seqs in seqs_by_month.items():\n",
    "        seqs.extend(month_seqs)\n",
    "        months.extend([month_val] * len(month_seqs))\n",
    "    \n",
    "    print(f\"Collected {len(seqs)} sequences across {len(seqs_by_month)} unique months\")\n",
    "    print(f\"Months: {sorted(seqs_by_month.keys())}\")\n",
    "    \n",
    "    return seqs, np.array(months).reshape(-1, 1)\n",
    "\n",
    "def mutate(seq_list, rate):\n",
    "    \"\"\"Randomize amino acids based on rate.\"\"\"\n",
    "    noised = []\n",
    "    for s in tqdm(seq_list, desc=f'mutating (rate={rate:.6f})', leave=False):\n",
    "        s_arr = np.array(list(s))\n",
    "        mask = np.random.rand(len(s_arr)) < rate\n",
    "        s_arr[mask] = np.random.choice(aas, size=mask.sum())\n",
    "        noised.append(\"\".join(s_arr))\n",
    "    return noised\n",
    "\n",
    "# Parse and split data\n",
    "print(\"Parsing data...\")\n",
    "all_seqs, all_months = parse_data(fn, lines_to_read=10**8, max_per_month=1000)\n",
    "tr_seqs, te_seqs, _, te_months = train_test_split(all_seqs, all_months, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(tr_seqs)}, Test: {len(te_seqs)}\")\n",
    "print(f\"Unique months in test set: {np.unique(te_months)}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "# Prepare data for each noise level\n",
    "print(\"\\nTokenizing and saving data for each noise level...\")\n",
    "for nl in tqdm(noise_levels, desc=\"Noise levels\"):\n",
    "    print(f\"\\nProcessing noise level: {nl:.6f}\")\n",
    "    \n",
    "    # Apply noise\n",
    "    tr_n = mutate(tr_seqs, nl)\n",
    "    te_n = mutate(te_seqs, nl)\n",
    "    \n",
    "    # Tokenize\n",
    "    print(\"  Tokenizing train data...\")\n",
    "    train_enc = tok(tr_n, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    print(\"  Tokenizing test data...\")\n",
    "    test_enc = tok(te_n, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    \n",
    "    # Save\n",
    "    data_dict = {\n",
    "        'train_input_ids': train_enc['input_ids'],\n",
    "        'train_attention_mask': train_enc['attention_mask'],\n",
    "        'test_input_ids': test_enc['input_ids'],\n",
    "        'test_attention_mask': test_enc['attention_mask'],\n",
    "        'test_seqs': te_n,\n",
    "        'test_months': te_months,\n",
    "        'noise_level': nl\n",
    "    }\n",
    "    \n",
    "    save_path = f\"seq/data_noise_{nl:.6f}.pt\"\n",
    "    torch.save(data_dict, save_path)\n",
    "    print(f\"  Saved to {save_path}\")\n",
    "\n",
    "print(\"\\data preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f900d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
