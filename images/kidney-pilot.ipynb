{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae26773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from latentmi import lmi, ksg\n",
    "import pandas as pd\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf098f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gauss(x: torch.Tensor, sigma: float = 0.1) -> torch.Tensor:\n",
    "    \"\"\"Add Gaussian noise to image.\"\"\"\n",
    "    return x + torch.randn_like(x) * sigma\n",
    "\n",
    "\n",
    "def pixelate(x: torch.Tensor, scale: int = 4) -> torch.Tensor:\n",
    "    \"\"\"Pixelate image by downsampling and upsampling.\"\"\"\n",
    "    h, w = x.shape[-2:]\n",
    "    small = F.avg_pool2d(x, kernel_size=scale, stride=scale)\n",
    "    return F.interpolate(small, size=(h, w), mode='nearest')\n",
    "\n",
    "\n",
    "def create_model(num_classes: int, device: torch.device) -> nn.Module:\n",
    "    \"\"\"Create MobileNetV3-small model adapted for 3-channel input.\"\"\"\n",
    "    model = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
    "    \n",
    "    # MobileNetV3 already expects 3 channels, but we'll modify classifier\n",
    "    model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, noise_fn: Optional[Callable],\n",
    "                device: torch.device, n_epochs: int = 50, lr: float = 1e-3) -> None:\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Epochs\", leave=True):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=False):\n",
    "            x, y = x.to(device), y.squeeze().long().to(device)\n",
    "            \n",
    "            # Apply noise if specified\n",
    "            if noise_fn:\n",
    "                x = noise_fn(x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {avg_loss:.4f} | Acc: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "def compute_mi(model: nn.Module, data_loader: DataLoader, noise_fn: Optional[Callable],\n",
    "               device: torch.device, num_classes: int) -> Tuple[float, float, dict]:\n",
    "    \"\"\"Compute mutual information between representations and labels.\n",
    "    \n",
    "    Returns:\n",
    "        mi_score: Continuous MI from representations\n",
    "        discrete_mi: Overall discrete MI\n",
    "        ova_mis: Dictionary mapping class indices to one-vs-all MIs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    representations = []\n",
    "    targets = []\n",
    "    y_hats = []\n",
    "    \n",
    "    # Create hook to extract features\n",
    "    features = {}\n",
    "    def get_features(name):\n",
    "        def hook(model, input, output):\n",
    "            features[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hook on last layer before classifier\n",
    "    model.classifier[0].register_forward_hook(get_features('last_layer'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.squeeze().long()\n",
    "            \n",
    "            # Apply noise if specified\n",
    "            if noise_fn:\n",
    "                x = noise_fn(x)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            representations.append(features['last_layer'].cpu())\n",
    "            targets.append(y)\n",
    "            y_hats.append(y_hat.cpu())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = torch.cat(representations).numpy()\n",
    "    Y = torch.cat(targets).numpy()\n",
    "    Y_onehot = np.eye(num_classes)[Y]\n",
    "    Y_hats_onehot = torch.cat(y_hats).numpy()\n",
    "    Y_hats = np.argmax(Y_hats_onehot, axis=1)\n",
    "    \n",
    "    print(f\"Representations shape: {X.shape}, Labels shape: {Y.shape}, Predictions shape: {Y_hats.shape}\")\n",
    "    \n",
    "    # Calculate MI using latentmi\n",
    "    pmis, _, _ = lmi.estimate(X, Y_onehot, validation_split=0.3, batch_size=512, \n",
    "                             epochs=50, quiet=False)\n",
    "    mi_score = np.nanmean(pmis)\n",
    "    \n",
    "    print(f\"MI score (representation): {mi_score:.4f}\")\n",
    "\n",
    "    discrete_mi = ksg.midd(Y, Y_hats)\n",
    "    print(f\"MI score (discrete): {discrete_mi:.4f}\")\n",
    "    \n",
    "    # Compute one-vs-all MIs with balanced resampling\n",
    "    print(\"\\nComputing one-vs-all MIs...\")\n",
    "    ova_mis = {}\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Create binary labels (1 for current class, 0 for all others)\n",
    "        Y_binary = (Y == class_idx).astype(int)\n",
    "        Y_hats_binary = (Y_hats == class_idx).astype(int)\n",
    "        \n",
    "        # Count samples in each binary class\n",
    "        n_positive = np.sum(Y_binary)\n",
    "        n_negative = len(Y_binary) - n_positive\n",
    "        \n",
    "        # Balanced resampling: take min of the two classes and sample equally\n",
    "        n_samples = min(n_positive, n_negative)\n",
    "        \n",
    "        if n_samples == 0:\n",
    "            print(f\"  Class_{class_idx}: skipped (no samples)\")\n",
    "            ova_mis[f\"Class_{class_idx}\"] = np.nan\n",
    "            continue\n",
    "        \n",
    "        positive_indices = np.where(Y_binary == 1)[0]\n",
    "        negative_indices = np.where(Y_binary == 0)[0]\n",
    "        \n",
    "        # Randomly sample n_samples from each class\n",
    "        np.random.seed(42)  \n",
    "        sampled_positive = np.random.choice(positive_indices, size=n_samples, replace=True)\n",
    "        sampled_negative = np.random.choice(negative_indices, size=n_samples, replace=True)\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        balanced_indices = np.concatenate([sampled_positive, sampled_negative])\n",
    "        np.random.shuffle(balanced_indices)\n",
    "        \n",
    "        # Get balanced labels and predictions\n",
    "        Y_binary_balanced = Y_binary[balanced_indices]\n",
    "        Y_hats_binary_balanced = Y_hats_binary[balanced_indices]\n",
    "        \n",
    "        # Compute MI for this one-vs-all binary classification\n",
    "        ova_mi = ksg.midd(Y_binary_balanced.reshape(-1, 1), Y_hats_binary_balanced.reshape(-1, 1))\n",
    "        ova_mis[f\"Class_{class_idx}\"] = ova_mi\n",
    "        \n",
    "        print(f\"  Class_{class_idx} (one-vs-all, balanced {n_samples} per class): {ova_mi:.4f} bits\")\n",
    "    \n",
    "    return mi_score, discrete_mi, ova_mis\n",
    "\n",
    "\n",
    "def run_experiment(dataset_name: str, output_dir: str, noise_fn: Optional[Callable],\n",
    "                   tag: str, device: torch.device, n_epochs: int = 50,\n",
    "                   size: int = 224, download: bool = True) -> Tuple[float, float, dict]:\n",
    "    \"\"\"Run a single noise experiment.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting experiment: {tag}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get dataset info\n",
    "    info = INFO[dataset_name]\n",
    "    num_classes = len(info['label'])\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Labels: {info['label']}\")\n",
    "    \n",
    "    # Prepare transforms - convert grayscale to RGB by repeating channel\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),  # Convert 1-channel to 3-channel\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_ds = DataClass(split='train', transform=tfm, download=download, size=size)\n",
    "    test_ds = DataClass(split='test', transform=tfm, download=download, size=size)\n",
    "    \n",
    "    train_dl = DataLoader(train_ds, batch_size=512, shuffle=True, num_workers=4)\n",
    "    test_dl = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(f'Dataset split: {len(train_ds)} train, {len(test_ds)} test')\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(num_classes, device)\n",
    "    \n",
    "    train_model(model, train_dl, noise_fn, device, n_epochs=n_epochs)\n",
    "    \n",
    "    # Save model\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model_path = os.path.join(output_dir, f\"model_{tag}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    # Compute MI\n",
    "    mi_score, discrete_mi, ova_mis = compute_mi(model, test_dl, noise_fn, device, num_classes)\n",
    "    \n",
    "    return mi_score, discrete_mi, ova_mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1ca1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============================================================\n",
      "Starting experiment: clean\n",
      "============================================================\n",
      "Dataset: tissuemnist\n",
      "Number of classes: 8\n",
      "Labels: {'0': 'Collecting Duct, Connecting Tubule', '1': 'Distal Convoluted Tubule', '2': 'Glomerular endothelial cells', '3': 'Interstitial endothelial cells', '4': 'Leukocytes', '5': 'Podocytes', '6': 'Proximal Tubule Segments', '7': 'Thick Ascending Limb'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orcd/home/002/gokulg/miniforge3/envs/scaling/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: 165466 train, 47280 test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48adbbc624c4576b279d47300b8fdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae6660311a74e1994d2fea33a9e562d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.9024 | Acc: 67.24%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ab966b17f34692a8db176912bf708e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.7276 | Acc: 73.79%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6a49d59da04c50aa80646882b60644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.6651 | Acc: 76.08%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8569e76eb7d44fe4b23e5080839631ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.6131 | Acc: 78.03%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd9cd52368e4f678a87648ea7d2b504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.5691 | Acc: 79.54%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8b5ac0f4d04734a360999c1eada17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.5215 | Acc: 81.23%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e431e764aa8401bb18c636346b863af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.4742 | Acc: 83.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3c38e4f0db44bbb9319ba2207548e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 0.4245 | Acc: 84.66%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f48511eaf04bfd9af9e98df56a25d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m noise_fn, tag \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         mi_score, discrete_mi, ova_mis = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnoise_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m         results[tag] = {\n\u001b[32m     33\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mmi_score\u001b[39m\u001b[33m'\u001b[39m: mi_score,\n\u001b[32m     34\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mdiscrete_mi\u001b[39m\u001b[33m'\u001b[39m: discrete_mi,\n\u001b[32m     35\u001b[39m             **{\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mova_mi_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ova_mis.items()}\n\u001b[32m     36\u001b[39m         }\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 198\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(dataset_name, output_dir, noise_fn, tag, device, n_epochs, size, download)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Create and train model\u001b[39;00m\n\u001b[32m    196\u001b[39m model = create_model(num_classes, device)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[32m    201\u001b[39m os.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, noise_fn, device, n_epochs, lr)\u001b[39m\n\u001b[32m     33\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     x, y = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y.squeeze().long().to(device)\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Apply noise if specified\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m noise_fn:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset_name = 'tissuemnist'\n",
    "output_dir = 'tissuemnist_models'\n",
    "epochs = 10\n",
    "size = 224\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "experiments = [\n",
    "    (None, \"clean\"),\n",
    "] + [\n",
    "    (lambda x, s=s: add_gauss(x, s), f\"gauss_{s:.6f}\")\n",
    "    for s in np.logspace(-3, 1, 10)\n",
    "] + [\n",
    "    (lambda x, sc=sc: pixelate(x, sc), f\"pix_{sc}x\")\n",
    "    for sc in [2, 4, 7, 8, 14, 16, 28, 32, 56, 112, 224]\n",
    "]\n",
    "\n",
    "# Run all experiments\n",
    "results = {}\n",
    "for noise_fn, tag in experiments:\n",
    "    try:\n",
    "        mi_score, discrete_mi, ova_mis = run_experiment(\n",
    "            dataset_name, \n",
    "            output_dir, \n",
    "            noise_fn, \n",
    "            tag, \n",
    "            device,\n",
    "            n_epochs=epochs,\n",
    "            size=size,\n",
    "            download=True\n",
    "        )\n",
    "        results[tag] = {\n",
    "            'mi_score': mi_score,\n",
    "            'discrete_mi': discrete_mi,\n",
    "            **{f'ova_mi_{k}': v for k, v in ova_mis.items()}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment {tag}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        results[tag] = {'mi_score': np.nan, 'discrete_mi': np.nan}\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame([\n",
    "    {'experiment': tag, **scores}\n",
    "    for tag, scores in results.items()\n",
    "])\n",
    "results_path = os.path.join(output_dir, 'results.csv')\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\nResults saved to {results_path}\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9616f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract noise levels and MIs\n",
    "gauss_results = {k: v for k, v in results.items() if k.startswith('gauss_')}\n",
    "pix_results = {k: v for k, v in results.items() if k.startswith('pix_')}\n",
    "\n",
    "if gauss_results:\n",
    "    gauss_sigmas = [float(k.split('_')[1]) for k in gauss_results.keys()]\n",
    "    gauss_mis = [v['mi_score'] for v in gauss_results.values()]\n",
    "    gauss_discrete_mis = [v['discrete_mi'] for v in gauss_results.values()]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.semilogx(gauss_sigmas, gauss_mis, 'o-', label='Representation MI')\n",
    "    ax1.semilogx(gauss_sigmas, gauss_discrete_mis, 's-', label='Discrete MI')\n",
    "    ax1.set_xlabel('Gaussian Noise Sigma')\n",
    "    ax1.set_ylabel('Mutual Information (bits)')\n",
    "    ax1.set_title('MI vs Gaussian Noise')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    if pix_results:\n",
    "        pix_scales = [int(k.split('_')[1].rstrip('x')) for k in pix_results.keys()]\n",
    "        pix_mis = [v['mi_score'] for v in pix_results.values()]\n",
    "        pix_discrete_mis = [v['discrete_mi'] for v in pix_results.values()]\n",
    "        \n",
    "        ax2.semilogx(pix_scales, pix_mis, 'o-', label='Representation MI')\n",
    "        ax2.semilogx(pix_scales, pix_discrete_mis, 's-', label='Discrete MI')\n",
    "        ax2.set_xlabel('Pixelation Scale')\n",
    "        ax2.set_ylabel('Mutual Information (bits)')\n",
    "        ax2.set_title('MI vs Pixelation')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'mi_scaling.png'), dpi=150)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
